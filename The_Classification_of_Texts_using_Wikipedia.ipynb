{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Amirhatamian/NLP/blob/main/The_Classification_of_Texts_using_Wikipedia.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries\n",
        "!pip install wikipedia nltk scikit-learn PyPDF2\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mt4cfLIQEryE",
        "outputId": "95113436-b724-4c8a-987a-4ed74ad5eba0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wikipedia in /usr/local/lib/python3.10/dist-packages (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (4.12.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (2.31.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2024.2.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->wikipedia) (2.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install Wikipedia-API"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GcATzxreCQua",
        "outputId": "7b831cb5-ec8c-4340-ddc0-624ecc90a64c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Wikipedia-API in /usr/local/lib/python3.10/dist-packages (0.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from Wikipedia-API) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->Wikipedia-API) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->Wikipedia-API) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->Wikipedia-API) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->Wikipedia-API) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2dyZWL0mgqUT",
        "outputId": "6618edfe-525d-4870-8352-14eafd067c3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "#Import Libraries and Download NLTK Data\n",
        "\n",
        "import re\n",
        "import string\n",
        "import numpy as np\n",
        "import wikipedia\n",
        "import wikipediaapi\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import pos_tag, word_tokenize\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.probability import FreqDist\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import PyPDF2\n",
        "import warnings\n",
        "\n",
        "# Suppress specific warnings from a library\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module='wikipedia')\n",
        "\n",
        "# Download NLTK data\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Wikipedia API for the English language\n",
        "wiki_wiki = wikipediaapi.Wikipedia('english')\n",
        "\n",
        "# Function to retrieve text from a Wikipedia page\n",
        "def get_wikipedia_text(page_title):\n",
        "    # Retrieve the page object for the given title\n",
        "    page = wiki_wiki.page(page_title)\n",
        "\n",
        "    # Check if the page exists\n",
        "    if not page.exists():\n",
        "        return None\n",
        "\n",
        "    # Return the text content of the page\n",
        "    return page.text\n"
      ],
      "metadata": {
        "id": "Uqlqr9pElj7D"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#POS Tag Converter Function\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN  # Default to noun if unknown"
      ],
      "metadata": {
        "id": "fdupV4i37Vu8"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_keywords(text):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = word_tokenize(text)\n",
        "    tagged_words = pos_tag(words)\n",
        "    filtered_words = [\n",
        "        lemmatizer.lemmatize(word.lower(), get_wordnet_pos(pos))\n",
        "        for word, pos in tagged_words\n",
        "        if word.isalnum() and word.lower() not in stop_words\n",
        "    ]\n",
        "    return filtered_words\n"
      ],
      "metadata": {
        "id": "2Geephex6u6I"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_nouns(text):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = word_tokenize(text)\n",
        "    tagged_words = pos_tag(words)\n",
        "    nouns = [\n",
        "        lemmatizer.lemmatize(word.lower(), get_wordnet_pos(pos))\n",
        "        for word, pos in tagged_words\n",
        "        if pos.startswith('N') and word.lower() not in stop_words and word.isalnum()\n",
        "    ]\n",
        "    return nouns\n"
      ],
      "metadata": {
        "id": "qD5XVvi71uqY"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_wikipedia_text(topic):\n",
        "    try:\n",
        "        # Get the summary of the Wikipedia page for the topic\n",
        "        summary = wikipedia.summary(topic)\n",
        "        return summary\n",
        "    except wikipedia.exceptions.DisambiguationError as e:\n",
        "        # Handle disambiguation errors by choosing the first option\n",
        "        return wikipedia.summary(e.options[0])\n",
        "    except wikipedia.exceptions.PageError:\n",
        "        # Handle page errors (e.g., page not found)\n",
        "        return None"
      ],
      "metadata": {
        "id": "8cePXfTl1uz6"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_top_nouns(topics, num_top_nouns=10):\n",
        "    all_nouns = []\n",
        "\n",
        "    for topic in topics:\n",
        "        text = get_wikipedia_text(topic)\n",
        "        if text:\n",
        "            all_nouns.extend(extract_nouns(text))\n",
        "\n",
        "    # Remove common stopwords from the list of nouns\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_nouns = [noun for noun in all_nouns if noun.lower() not in stop_words]\n",
        "\n",
        "    # Create a frequency distribution of the nouns\n",
        "    nouns_freq_dist = FreqDist(filtered_nouns)\n",
        "    # Get the most common nouns\n",
        "    top_nouns = [word for word, _ in nouns_freq_dist.most_common(num_top_nouns)]\n",
        "\n",
        "    return top_nouns"
      ],
      "metadata": {
        "id": "CEYGUM053lPl"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample annotated keywords for geographic and non_geographic topics\n",
        "\n",
        "geographic_topics = ['New York', 'Mount Everest', 'Sahara Desert', 'Amazon River', 'Paris']\n",
        "non_geographic_topics = ['Quantum Mechanics', 'Artificial Intelligence', 'Shakespeare', 'Modern Art', 'Jazz Music']\n"
      ],
      "metadata": {
        "id": "m4xdgP8Q1u2f"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_geographic_keywords = extract_top_nouns(geographic_topics, num_top_nouns=10)\n",
        "top_non_geographic_keywords = extract_top_nouns(non_geographic_topics, num_top_nouns=10)\n",
        "\n",
        "print(\"Top geographic keywords:\", top_geographic_keywords)\n",
        "print(\"Top non-geographic keywords:\", top_non_geographic_keywords)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gYwWnvnk1u5M",
        "outputId": "8bfe1a51-b423-4e1b-b51b-d16bb5b4368a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top geographic keywords: ['state', 'river', 'new', 'city', 'world', 'york', 'paris', 'desert', 'area', 'region']\n",
            "Top non-geographic keywords: ['ai', 'jazz', 'art', 'quantum', 'shakespeare', 'work', 'theory', 'system', 'intelligence', 'physic']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine all topics\n",
        "all_topics = geographic_topics + non_geographic_topics\n",
        "all_docs = []\n",
        "all_labels = []\n",
        "\n",
        "# Fetch text and extract nouns for each topic\n",
        "for topic in all_topics:\n",
        "    text = get_wikipedia_text(topic)\n",
        "    if text:\n",
        "        nouns = extract_nouns(text)\n",
        "        all_docs.append(\" \".join(nouns))\n",
        "        # Label as 1 for geographic and 0 for non-geographic\n",
        "        all_labels.append(1 if topic in geographic_topics else 0)\n",
        "\n",
        "# Combine top keywords from both categories\n",
        "all_top_keywords = top_geographic_keywords + top_non_geographic_keywords\n",
        "\n",
        "# Vectorize the documents using the top keywords\n",
        "vectorizer = CountVectorizer(vocabulary=all_top_keywords)\n",
        "X = vectorizer.transform(all_docs)\n",
        "y = all_labels\n",
        "\n",
        "print(\"Feature matrix shape:\", X.shape)\n",
        "print(\"Labels:\", y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FqlMtd1Z1vD8",
        "outputId": "d99f7e1d-71f4-4aed-bbb1-1f9f940543c9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature matrix shape: (9, 20)\n",
            "Labels: [1, 1, 1, 1, 0, 0, 0, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "7Nw4fejY1vGe"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Majority class naive classifier\n",
        "class NaiveClassifier:\n",
        "    def __init__(self):\n",
        "        self.majority_class = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        unique_classes, counts = np.unique(y, return_counts=True)\n",
        "        self.majority_class = unique_classes[np.argmax(counts)]\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.full(X.shape[0], self.majority_class)\n",
        "\n",
        "# Instantiate and train the naive classifier\n",
        "naive_classifier = NaiveClassifier()\n",
        "naive_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "naive_predictions = naive_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, naive_predictions)\n",
        "report = classification_report(y_test, naive_predictions, target_names=['non-geographic', 'geographic'], zero_division=0)\n",
        "\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "print('Classification Report:')\n",
        "print(report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wd2mWTYr1vJQ",
        "outputId": "32989404-8532-448e-f751-ee280cd41a6f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 50.00%\n",
            "Classification Report:\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "non-geographic       0.50      1.00      0.67         1\n",
            "    geographic       0.00      0.00      0.00         1\n",
            "\n",
            "      accuracy                           0.50         2\n",
            "     macro avg       0.25      0.50      0.33         2\n",
            "  weighted avg       0.25      0.50      0.33         2\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Instantiate the logistic regression model with a specific random state for reproducibility\n",
        "logistic_model = LogisticRegression(random_state=42)\n",
        "\n",
        "# Train the logistic regression model on the training data\n",
        "logistic_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "logistic_predictions = logistic_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model's performance\n",
        "accuracy = accuracy_score(y_test, logistic_predictions)\n",
        "report = classification_report(y_test, logistic_predictions, zero_division=0)\n",
        "\n",
        "# Print performance metrics\n",
        "print(\"Logistic Regression Performance:\")\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(\"Classification Report:\")\n",
        "print(report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KbXV88D8CZkX",
        "outputId": "e2af25d2-5366-47bf-e370-a38a796927d2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Performance:\n",
            "Accuracy: 50.00%\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.50      1.00      0.67         1\n",
            "           1       0.00      0.00      0.00         1\n",
            "\n",
            "    accuracy                           0.50         2\n",
            "   macro avg       0.25      0.50      0.33         2\n",
            "weighted avg       0.25      0.50      0.33         2\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_pdf(pdf_file_path, vectorizer, classifier):\n",
        "    # Try-except block for error handling\n",
        "    try:\n",
        "        pdf_text = ''  # Initialize pdf_text to accumulate text from all pages\n",
        "        with open(pdf_file_path, 'rb') as file:\n",
        "            pdf_reader = PyPDF2.PdfReader(file)\n",
        "            for page in pdf_reader.pages:\n",
        "                text = page.extract_text()\n",
        "                if text:\n",
        "                    pdf_text += text\n",
        "            print(\"Extracted text from PDF:\", pdf_text)  # Debugging print statement\n",
        "\n",
        "        # Check if pdf_text is not empty\n",
        "        if not pdf_text:\n",
        "            return \"No extractable text found in the PDF.\"\n",
        "\n",
        "        # Preprocess and vectorize the PDF text\n",
        "        document_vectorized = vectorizer.transform([pdf_text])  # Ensure text is in a list format\n",
        "\n",
        "        # Predict the class of the PDF document\n",
        "        prediction = classifier.predict(document_vectorized)\n",
        "        return 'The document is classified as geographic.' if prediction[0] == 1 else 'The document is classified as non-geographic.'\n",
        "    except Exception as e:\n",
        "        return f\"An error occurred: {e}\""
      ],
      "metadata": {
        "id": "k2zVhgzeJ_J8"
      },
      "execution_count": 23,
      "outputs": []
    }
  ]
}