{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO/TpZMYeTH7EAvnKLKeoCW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Amirhatamian/NLP/blob/main/NLP_ASSIGNMENT1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install Wikipedia-API\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vzO0VD2_ll7z",
        "outputId": "f2ac79b6-7e5b-457f-cb44-0606148858e8"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Wikipedia-API in /usr/local/lib/python3.10/dist-packages (0.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from Wikipedia-API) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->Wikipedia-API) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->Wikipedia-API) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->Wikipedia-API) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->Wikipedia-API) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install wikipedia\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HoLXNGHQlwvA",
        "outputId": "da3f7818-e5c1-43d2-c8c5-66d43abf5ba4"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wikipedia in /usr/local/lib/python3.10/dist-packages (1.4.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (4.12.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2024.2.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->wikipedia) (2.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install PyPDF2\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MD44Ozhb7FGt",
        "outputId": "013cbf7d-e711-4550-8cdc-96e6d325eb93"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.10/dist-packages (3.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "2dyZWL0mgqUT"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import wikipediaapi\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag, FreqDist\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import wikipedia\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Wikipedia API for the English language\n",
        "wiki_wiki = wikipediaapi.Wikipedia('english')\n",
        "\n",
        "# Function to retrieve text from a Wikipedia page\n",
        "def get_wikipedia_text(page_title):\n",
        "    # Retrieve the page object for the given title\n",
        "    page = wiki_wiki.page(page_title)\n",
        "\n",
        "    # Check if the page exists\n",
        "    if not page.exists():\n",
        "        return None\n",
        "\n",
        "    # Return the text content of the page\n",
        "    return page.text\n"
      ],
      "metadata": {
        "id": "Uqlqr9pElj7D"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# download the required NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def extract_keywords(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = word_tokenize(text)\n",
        "    filtered_words = [word.lower() for word in words if word.isalnum() and word.lower() not in stop_words]\n",
        "    return filtered_words\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Geephex6u6I",
        "outputId": "4d9322d2-327f-40e1-c3dc-94533840e7c3"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_nouns(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = word_tokenize(text)\n",
        "    tagged_words = pos_tag(words)\n",
        "    nouns = [word.lower() for word, pos in tagged_words if pos.startswith('N') and word.lower() not in stop_words and word.isalnum()]\n",
        "    return nouns\n"
      ],
      "metadata": {
        "id": "qD5XVvi71uqY"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def get_wikipedia_text(topic):\n",
        "    try:\n",
        "        # Get the summary of the Wikipedia page for the topic\n",
        "        summary = wikipedia.summary(topic)\n",
        "        return summary\n",
        "    except wikipedia.exceptions.DisambiguationError as e:\n",
        "        # Handle disambiguation errors by choosing the first option\n",
        "        return wikipedia.summary(e.options[0])\n",
        "    except wikipedia.exceptions.PageError:\n",
        "        # Handle page errors (e.g., page not found)\n",
        "        return None\n",
        "\n",
        "def extract_nouns(text):\n",
        "    # Tokenize the text into words\n",
        "    words = word_tokenize(text)\n",
        "    # Tag words with part of speech\n",
        "    pos_tags = nltk.pos_tag(words)\n",
        "    # Extract nouns (NN, NNS, NNP, NNPS are the tags for different types of nouns)\n",
        "    nouns = [word for word, pos in pos_tags if pos in ['NN', 'NNS', 'NNP', 'NNPS']]\n",
        "    return nouns\n",
        "\n",
        "def extract_top_nouns(topics, num_top_nouns=10):\n",
        "    all_nouns = []\n",
        "\n",
        "    for topic in topics:\n",
        "        text = get_wikipedia_text(topic)\n",
        "        if text:\n",
        "            all_nouns.extend(extract_nouns(text))\n",
        "\n",
        "    # Remove common stopwords from the list of nouns\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_nouns = [noun for noun in all_nouns if noun.lower() not in stop_words]\n",
        "\n",
        "    # Create a frequency distribution of the nouns\n",
        "    nouns_freq_dist = FreqDist(filtered_nouns)\n",
        "    # Get the most common nouns\n",
        "    top_nouns = [word for word, _ in nouns_freq_dist.most_common(num_top_nouns)]\n",
        "\n",
        "    return top_nouns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cePXfTl1uz6",
        "outputId": "16c7ac2b-59cc-4f82-cefc-5e12ee4fb372"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['AI', 'language', 'intelligence', 'research', 'goals', 'processing', 'computer', 'field', 'e.g.', 'science']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "geographic_topics = ['New York', 'Mount Everest', 'Sahara Desert', 'Amazon River', 'Paris']\n",
        "non_geographic_topics = ['Quantum Mechanics', 'Artificial Intelligence', 'Shakespeare', 'Modern Art', 'Jazz Music']\n"
      ],
      "metadata": {
        "id": "m4xdgP8Q1u2f"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_geographic_keywords = extract_top_nouns(geographic_topics, num_top_nouns=10)\n",
        "top_non_geographic_keywords = extract_top_nouns(non_geographic_topics, num_top_nouns=10)\n",
        "\n",
        "print(\"Top geographic keywords:\", top_geographic_keywords)\n",
        "print(\"Top non-geographic keywords:\", top_non_geographic_keywords)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gYwWnvnk1u5M",
        "outputId": "22ba9427-2180-409b-936c-363fa9aedae3"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
            "\n",
            "The code that caused this warning is on line 389 of the file /usr/local/lib/python3.10/dist-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
            "\n",
            "  lis = BeautifulSoup(html).find_all('li')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top geographic keywords: ['New', 'York', 'Paris', 'city', 'world', 'state', 'River', 'Amazon', 'United', 'area']\n",
            "Top non-geographic keywords: ['AI', 'jazz', 'art', 'Shakespeare', 'systems', 'intelligence', 'works', 'physics', 'quantum', 'mechanics']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine all topics\n",
        "all_topics = geographic_topics + non_geographic_topics\n",
        "all_docs = []\n",
        "all_labels = []\n",
        "\n",
        "# Fetch text and extract nouns for each topic\n",
        "for topic in all_topics:\n",
        "    text = get_wikipedia_text(topic)\n",
        "    if text:\n",
        "        nouns = extract_nouns(text)\n",
        "        all_docs.append(\" \".join(nouns))\n",
        "        # Label as 1 for geographic and 0 for non-geographic\n",
        "        all_labels.append(1 if topic in geographic_topics else 0)\n",
        "\n",
        "# Combine top keywords from both categories\n",
        "all_top_keywords = top_geographic_keywords + top_non_geographic_keywords\n",
        "\n",
        "# Vectorize the documents using the top keywords\n",
        "vectorizer = CountVectorizer(vocabulary=all_top_keywords)\n",
        "X = vectorizer.transform(all_docs)\n",
        "y = all_labels\n",
        "\n",
        "print(\"Feature matrix shape:\", X.shape)\n",
        "print(\"Labels:\", y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FqlMtd1Z1vD8",
        "outputId": "9ec83514-b040-4a53-977e-bf8afe850b64"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature matrix shape: (9, 20)\n",
            "Labels: [1, 1, 1, 1, 0, 0, 0, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "7Nw4fejY1vGe"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Majority class naive classifier\n",
        "class NaiveClassifier:\n",
        "    def __init__(self):\n",
        "        self.majority_class = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        unique_classes, counts = np.unique(y, return_counts=True)\n",
        "        self.majority_class = unique_classes[np.argmax(counts)]\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.full(X.shape[0], self.majority_class)\n",
        "\n",
        "# Instantiate and train the naive classifier\n",
        "naive_classifier = NaiveClassifier()\n",
        "naive_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "naive_predictions = naive_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, naive_predictions)\n",
        "report = classification_report(y_test, naive_predictions, target_names=['non-geographic', 'geographic'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wd2mWTYr1vJQ",
        "outputId": "9bc14849-3f59-4b67-dfc9-b422fda6645b"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the path to the PDF file on your desktop\n",
        "pdf_file_path = \"path_to_your_pdf_file.pdf\"\n",
        "\n",
        "# Read the content of the PDF file\n",
        "with open(pdf_file_path, 'rb') as file:\n",
        "    pdf_reader = PyPDF2.PdfReader(file)\n",
        "    pdf_text = ''\n",
        "    for page_num in range(len(pdf_reader.pages)):\n",
        "        page = pdf_reader.pages[page_num]\n",
        "        pdf_text += page.extract_text()\n",
        "\n",
        "# Vectorize the PDF document\n",
        "document_vectorized = vectorize_document(pdf_text, vectorizer)\n",
        "\n",
        "# Predict the class of the PDF document\n",
        "naive_prediction = naive_classifier.predict(document_vectorized)\n",
        "\n",
        "if naive_prediction[0] == 1:\n",
        "    print('The document is classified as geographic.')\n",
        "else:\n",
        "    print('The document is classified as non-geographic.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "0SVdZr1Z_f12",
        "outputId": "f4eda699-7c7f-4ad1-a730-0253bf197a83"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'path_to_your_pdf_file.pdf'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-fa6127efe296>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Read the content of the PDF file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdf_file_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mpdf_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPyPDF2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPdfReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mpdf_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'path_to_your_pdf_file.pdf'"
          ]
        }
      ]
    }
  ]
}
