{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNLh9CecNsFlpP5e6mPhGkO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Amirhatamian/NLP/blob/main/LLM%20Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install wikipedia-api\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_UwtIlViAmX",
        "outputId": "d448cab5-d07c-44fd-f7e5-0f04ee164eb5"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wikipedia-api in /usr/local/lib/python3.10/dist-packages (0.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from wikipedia-api) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rHjMyCLhd7n0",
        "outputId": "23946f67-af8a-4e23-9fdf-dbc7e65f0426"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import wikipediaapi\n",
        "\n",
        "# Ensure required NLTK data is downloaded\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_wikipedia_content(subject):\n",
        "    # Initialize the Wikipedia API with the English language\n",
        "    wiki_wiki = wikipediaapi.Wikipedia('english')\n",
        "\n",
        "    # Fetch the Wikipedia page for the given subject\n",
        "    page_py = wiki_wiki.page(subject)\n",
        "\n",
        "    # Check if the page exists\n",
        "    if not page_py.exists():\n",
        "        print(f\"Wikipedia page for '{subject}' does not exist.\")\n",
        "        return None\n",
        "\n",
        "    # Return the text content of the page\n",
        "    return page_py.text\n"
      ],
      "metadata": {
        "id": "5_GyATZseWEV"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_content_to_file(content, filename):\n",
        "    # Check if the content is not None\n",
        "    if content is None:\n",
        "        print(\"No content to save.\")\n",
        "        return\n",
        "\n",
        "    # Write the content to a file\n",
        "    with open(filename, 'w', encoding='utf-8') as file:\n",
        "        file.write(content)\n",
        "\n",
        "    print(f\"Content saved to {filename}\")\n"
      ],
      "metadata": {
        "id": "LSMofwwceWHF"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_content_from_file(filename):\n",
        "    try:\n",
        "        # Open the file and read its content\n",
        "        with open(filename, 'r', encoding='utf-8') as file:\n",
        "            content = file.read()\n",
        "        return content\n",
        "    except FileNotFoundError:\n",
        "        print(f\"File '{filename}' not found.\")\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "ygTyNnYDeWJy"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_text(text, max_sentences=5):\n",
        "    \"\"\"\n",
        "    Summarize the text to fit within the context window.\n",
        "\n",
        "    :param text: The text to be summarized.\n",
        "    :param max_sentences: The maximum number of sentences for the summary.\n",
        "    :return: The summarized text.\n",
        "    \"\"\"\n",
        "    # Tokenize the text into sentences\n",
        "    sentences = sent_tokenize(text)\n",
        "\n",
        "    # If the text is already short enough, return it as is\n",
        "    if len(sentences) <= max_sentences:\n",
        "        return text\n",
        "\n",
        "    # Simple summarization: take the first and last few sentences\n",
        "    summary = sentences[:max_sentences//2] + sentences[-max_sentences//2:]\n",
        "\n",
        "    return ' '.join(summary)\n"
      ],
      "metadata": {
        "id": "VR_u22pxeWMU"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def perform_summarization(content, output_filename, context_window_limit):\n",
        "    \"\"\"\n",
        "    Summarize content to fit within the context window limit and save to file.\n",
        "\n",
        "    :param content: The text content to be summarized.\n",
        "    :param output_filename: The name of the file to save the summary.\n",
        "    :param context_window_limit: The maximum word count for the context window.\n",
        "    :return: The summarized text.\n",
        "    \"\"\"\n",
        "    length = len(content.split())\n",
        "    target_length = int(length * (context_window_limit / (context_window_limit + 4000)))\n",
        "\n",
        "    summary_list = []\n",
        "    start_index = 0\n",
        "    content_words = content.split()\n",
        "\n",
        "    while start_index < length:\n",
        "        end_index = start_index + target_length\n",
        "        slice_content = \" \".join(content_words[start_index:end_index])\n",
        "        summary_slice = summarize_text(slice_content, max_sentences=target_length//50)\n",
        "        summary_list.append(summary_slice)\n",
        "        start_index = end_index\n",
        "\n",
        "    collated_summary = \" \".join(summary_list)\n",
        "\n",
        "    while len(collated_summary.split()) > context_window_limit:\n",
        "        collated_summary = summarize_text(collated_summary, max_sentences=context_window_limit//50)\n",
        "\n",
        "    with open(output_filename, \"w\", encoding=\"utf-8\") as file:\n",
        "        file.write(collated_summary)\n",
        "\n",
        "    return collated_summary\n"
      ],
      "metadata": {
        "id": "H04YQA5PeWO2"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main function\n",
        "def main():\n",
        "    # Take input from the user for the Wikipedia subjects\n",
        "    subject1 = input(\"Enter the first subject: \")\n",
        "    subject2 = input(\"Enter the second subject: \")\n",
        "\n",
        "    # Fetch content from Wikipedia\n",
        "    content1 = fetch_wikipedia_content(subject1)\n",
        "    content2 = fetch_wikipedia_content(subject2)\n",
        "\n",
        "    if content1 and content2:\n",
        "        # Save content to input files\n",
        "        save_content_to_file(content1, \"input_text1.txt\")\n",
        "        save_content_to_file(content2, \"input_text2.txt\")\n",
        "\n",
        "        # Summarize text files\n",
        "        summarized_content1 = perform_summarization(content1, \"summarized_text1.txt\", 128)\n",
        "        summarized_content2 = perform_summarization(content2, \"summarized_text2.txt\", 128)\n",
        "\n",
        "        # Generate the query\n",
        "        query = f\"\\nDocument 1 summary: {summarized_content1}\\n\\nDocument 2 summary: {summarized_content2}\"\n",
        "\n",
        "        print(query)"
      ],
      "metadata": {
        "id": "T4_hcHAKeWRN"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the main function\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "Z844M-SO9SXd",
        "outputId": "385b5cbb-9964-4fd3-89b8-64b3a99a681d"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the first subject: Natural Language Processing\n",
            "Enter the second subject: Data Science\n",
            "Content saved to input_text1.txt\n",
            "Content saved to input_text2.txt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-b4d031890515>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Run the main function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-52-60012e2bdbcf>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# Summarize text files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0msummarized_content1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperform_summarization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"summarized_text1.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0msummarized_content2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperform_summarization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"summarized_text2.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# Generate the query\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-51-b4dda0356a7b>\u001b[0m in \u001b[0;36mperform_summarization\u001b[0;34m(content, output_filename, context_window_limit)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mcollated_summary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcollated_summary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mcontext_window_limit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mcollated_summary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msummarize_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcollated_summary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_sentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontext_window_limit\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "Df9rWjxf-wiW"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_similarity_matrix(sentences):\n",
        "    \"\"\"\n",
        "    Compute the similarity matrix for a list of sentences.\n",
        "\n",
        "    :param sentences: A list of sentences.\n",
        "    :return: A similarity matrix.\n",
        "    \"\"\"\n",
        "    # Transform sentences into a matrix of token counts\n",
        "    vectorizer = CountVectorizer().fit_transform(sentences)\n",
        "\n",
        "    # Compute cosine similarity between vectors\n",
        "    similarity_matrix = cosine_similarity(vectorizer)\n",
        "\n",
        "    return similarity_matrix\n"
      ],
      "metadata": {
        "id": "E-smbEIC-8Xs"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_text_cosine_similarity(text, target_length):\n",
        "    sentences = sent_tokenize(text)\n",
        "\n",
        "    if len(sentences) == 0:\n",
        "        return \"\"\n",
        "\n",
        "    # Compute cosine similarity matrix between sentences\n",
        "    similarity_matrix = compute_similarity_matrix(sentences)\n",
        "\n",
        "    length = 0\n",
        "    summary_sentences = []\n",
        "    selected_indices = set()\n",
        "\n",
        "    while length < target_length and len(selected_indices) < len(sentences):\n",
        "        if len(summary_sentences) == 0:\n",
        "            # If summary is empty, start with the first sentence\n",
        "            most_similar_index = 0\n",
        "        else:\n",
        "            # Find the sentence most similar to the existing summary\n",
        "            summary_indices = [i for i in range(len(sentences)) if i in selected_indices]\n",
        "            similarity_scores = similarity_matrix[summary_indices].sum(axis=0)\n",
        "            similarity_scores[list(selected_indices)] = 0  # Avoid selecting already chosen sentences\n",
        "            most_similar_index = np.argmax(similarity_scores)\n",
        "\n",
        "        if most_similar_index in selected_indices:\n",
        "            break\n",
        "\n",
        "        # Add the selected sentence to the summary\n",
        "        summary_sentences.append(sentences[most_similar_index])\n",
        "        selected_indices.add(most_similar_index)\n",
        "\n",
        "        # Update the length based on the added sentence\n",
        "        length += len(sentences[most_similar_index].split())\n",
        "\n",
        "    return ' '.join(summary_sentences).strip()"
      ],
      "metadata": {
        "id": "5hLx3X-n-8wT"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def perform_summarization(content, output_filename, context_window_limit):\n",
        "    length = len(content.split())\n",
        "    target_length = int(length * (context_window_limit / (context_window_limit + 4000)))\n",
        "\n",
        "    summary = summarize_text_cosine_similarity(content, target_length)\n",
        "\n",
        "    with open(output_filename, \"w\", encoding=\"utf-8\") as file:\n",
        "        file.write(summary)\n",
        "\n",
        "    return summary"
      ],
      "metadata": {
        "id": "QK3GacquAHRF"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Reading the content from input files\n",
        "    contents_1 = read_content_from_file(\"input_text1.txt\")\n",
        "    contents_2 = read_content_from_file(\"input_text2.txt\")\n",
        "\n",
        "    if contents_1 and contents_2:\n",
        "        # Perform summarization on both files\n",
        "        perform_summarization(contents_1, \"Second_summarized_text1.txt\", 128)\n",
        "        perform_summarization(contents_2, \"Second_summarized_text2.txt\", 128)\n",
        "\n",
        "        # Generate the query with the summarized content\n",
        "        query = f\"\\nDocument 1 summary: {read_content_from_file('Second_summarized_text1.txt')}\\n\" \\\n",
        "                f\"\\nDocument 2 summary: {read_content_from_file('Second_summarized_text2.txt')}\"\n",
        "\n",
        "        print(query)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYmrYWBtBXP9",
        "outputId": "c7bca742-c88e-4d86-8c54-c2f28cd28b37"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Document 1 summary: Natural language processing (NLP) is an interdisciplinary subfield of computer science and information retrieval. Book generation\n",
            "Not an NLP task proper but an extension of natural language generation and other NLP tasks is the creation of full-fledged books. Common NLP tasks\n",
            "The following is a list of some of the most commonly researched tasks in natural language processing. As an example, George Lakoff offers a methodology to build natural language processing (NLP) algorithms through the perspective of cognitive science, along with the findings of cognitive linguistics, with two defining aspects:\n",
            "\n",
            "Apply the theory of conceptual metaphor, explained by Lakoff as \"the understanding of one idea, in terms of another\" which provides an idea of the intent of the author. The goal is a computer capable of \"understanding\" the contents of documents, including the contextual nuances of the language within them. The difficulty of this task depends greatly on the complexity of the morphology (i.e., the structure of words) of the language being considered.\n",
            "\n",
            "Document 2 summary: Data science is an interdisciplinary academic field that uses statistics, scientific computing, scientific methods, processes, scientific visualization, algorithms and systems to extract or extrapolate knowledge and insights from potentially noisy, structured, or unstructured data. Foundations\n",
            "Data science is an interdisciplinary field focused on extracting knowledge from typically large data sets and applying the knowledge and insights from that data to solve problems in a wide range of application domains.\n"
          ]
        }
      ]
    }
  ]
}