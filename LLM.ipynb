{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNLow4ipvChW3hkzkGa8fMH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Amirhatamian/NLP/blob/main/LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install wikipedia-api\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_UwtIlViAmX",
        "outputId": "af67cfec-2ef7-4f16-889a-bd462b56b8c7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wikipedia-api\n",
            "  Downloading Wikipedia_API-0.6.0-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from wikipedia-api) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (2024.2.2)\n",
            "Installing collected packages: wikipedia-api\n",
            "Successfully installed wikipedia-api-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rHjMyCLhd7n0",
        "outputId": "0a46777d-0436-4a2b-a6e3-90188dd1298d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import wikipediaapi\n",
        "\n",
        "# Ensure required NLTK data is downloaded\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_wikipedia_content(subject):\n",
        "    # Initialize the Wikipedia API with the English language\n",
        "    wiki_wiki = wikipediaapi.Wikipedia('english')\n",
        "\n",
        "    # Fetch the Wikipedia page for the given subject\n",
        "    page_py = wiki_wiki.page(subject)\n",
        "\n",
        "    # Check if the page exists\n",
        "    if not page_py.exists():\n",
        "        print(f\"Wikipedia page for '{subject}' does not exist.\")\n",
        "        return None\n",
        "\n",
        "    # Return the text content of the page\n",
        "    return page_py.text\n"
      ],
      "metadata": {
        "id": "5_GyATZseWEV"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_content_to_file(content, filename):\n",
        "    # Check if the content is not None\n",
        "    if content is None:\n",
        "        print(\"No content to save.\")\n",
        "        return\n",
        "\n",
        "    # Write the content to a file\n",
        "    with open(filename, 'w', encoding='utf-8') as file:\n",
        "        file.write(content)\n",
        "\n",
        "    print(f\"Content saved to {filename}\")\n"
      ],
      "metadata": {
        "id": "LSMofwwceWHF"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_content_from_file(filename):\n",
        "    try:\n",
        "        # Open the file and read its content\n",
        "        with open(filename, 'r', encoding='utf-8') as file:\n",
        "            content = file.read()\n",
        "        return content\n",
        "    except FileNotFoundError:\n",
        "        print(f\"File '{filename}' not found.\")\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "ygTyNnYDeWJy"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_text(text, max_sentences=5):\n",
        "    \"\"\"\n",
        "    Summarize the text to fit within the context window.\n",
        "\n",
        "    :param text: The text to be summarized.\n",
        "    :param max_sentences: The maximum number of sentences for the summary.\n",
        "    :return: The summarized text.\n",
        "    \"\"\"\n",
        "    # Tokenize the text into sentences\n",
        "    sentences = sent_tokenize(text)\n",
        "\n",
        "    # If the text is already short enough, return it as is\n",
        "    if len(sentences) <= max_sentences:\n",
        "        return text\n",
        "\n",
        "    # Simple summarization: take the first and last few sentences\n",
        "    summary = sentences[:max_sentences//2] + sentences[-max_sentences//2:]\n",
        "\n",
        "    return ' '.join(summary)\n"
      ],
      "metadata": {
        "id": "VR_u22pxeWMU"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def perform_summarization(content, output_filename, context_window_limit):\n",
        "    \"\"\"\n",
        "    Summarize content to fit within the context window limit and save to file.\n",
        "\n",
        "    :param content: The text content to be summarized.\n",
        "    :param output_filename: The name of the file to save the summary.\n",
        "    :param context_window_limit: The maximum word count for the context window.\n",
        "    :return: The summarized text.\n",
        "    \"\"\"\n",
        "    length = len(content.split())\n",
        "    target_length = int(length * (context_window_limit / (context_window_limit + 4000)))\n",
        "\n",
        "    summary_list = []\n",
        "    start_index = 0\n",
        "    content_words = content.split()\n",
        "\n",
        "    while start_index < length:\n",
        "        end_index = start_index + target_length\n",
        "        slice_content = \" \".join(content_words[start_index:end_index])\n",
        "        summary_slice = summarize_text(slice_content, max_sentences=target_length//50)\n",
        "        summary_list.append(summary_slice)\n",
        "        start_index = end_index\n",
        "\n",
        "    collated_summary = \" \".join(summary_list)\n",
        "\n",
        "    while len(collated_summary.split()) > context_window_limit:\n",
        "        collated_summary = summarize_text(collated_summary, max_sentences=context_window_limit//50)\n",
        "\n",
        "    with open(output_filename, \"w\", encoding=\"utf-8\") as file:\n",
        "        file.write(collated_summary)\n",
        "\n",
        "    return collated_summary\n"
      ],
      "metadata": {
        "id": "H04YQA5PeWO2"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main function\n",
        "def main():\n",
        "    # Take input from the user for the Wikipedia subjects\n",
        "    subject1 = input(\"Enter the first subject: \")\n",
        "    subject2 = input(\"Enter the second subject: \")\n",
        "\n",
        "    # Fetch content from Wikipedia\n",
        "    content1 = fetch_wikipedia_content(subject1)\n",
        "    content2 = fetch_wikipedia_content(subject2)\n",
        "\n",
        "    if content1 and content2:\n",
        "        # Save content to input files\n",
        "        save_content_to_file(content1, \"input_text1.txt\")\n",
        "        save_content_to_file(content2, \"input_text2.txt\")\n",
        "\n",
        "        # Summarize text files\n",
        "        summarized_content1 = perform_summarization(content1, \"summarized_text1.txt\", 128)\n",
        "        summarized_content2 = perform_summarization(content2, \"summarized_text2.txt\", 128)\n",
        "\n",
        "        # Generate the query\n",
        "        query = f\"\\nDocument 1 summary: {summarized_content1}\\n\\nDocument 2 summary: {summarized_content2}\"\n",
        "\n",
        "        print(query)\n",
        "\n"
      ],
      "metadata": {
        "id": "T4_hcHAKeWRN"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the main function\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z844M-SO9SXd",
        "outputId": "a7fa5685-edf3-4a91-e704-2e80e9c507b1"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Document 1 summary: Natural language processing (NLP) is an interdisciplinary subfield of computer science and information retrieval. Book generation\n",
            "Not an NLP task proper but an extension of natural language generation and other NLP tasks is the creation of full-fledged books. Common NLP tasks\n",
            "The following is a list of some of the most commonly researched tasks in natural language processing. As an example, George Lakoff offers a methodology to build natural language processing (NLP) algorithms through the perspective of cognitive science, along with the findings of cognitive linguistics, with two defining aspects:\n",
            "\n",
            "Apply the theory of conceptual metaphor, explained by Lakoff as \"the understanding of one idea, in terms of another\" which provides an idea of the intent of the author. The goal is a computer capable of \"understanding\" the contents of documents, including the contextual nuances of the language within them. The difficulty of this task depends greatly on the complexity of the morphology (i.e., the structure of words) of the language being considered.\n",
            "\n",
            "Document 2 summary: Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalize to unseen data, and thus perform tasks without explicit instructions. While responsible collection of data and documentation of algorithmic rules used by a system is considered a critical part of machine learning, some researchers blame lack of participation and representation of minority population in the field of AI for machine learning's vulnerability to biases. Reasons for this are numerous: lack of (suitable) data, lack of access to the data, data bias, privacy problems, badly chosen tasks and algorithms, wrong tools and people, lack of resources, and evaluation problems. Running machine learning model in embedded devices removes the need for transferring and storing data on cloud servers for further processing, henceforth, reducing data breaches and privacy leaks happening because of transferring data, and also minimizes theft of intellectual properties, personal data and business secrets. Instead of responding to feedback, unsupervised learning algorithms identify commonalities in the data and react based on the presence or absence of such commonalities in each new piece of data. Data compression\n",
            "Data mining\n",
            "Machine learning and data mining often employ the same methods and overlap significantly, but while machine learning focuses on prediction, based on known properties learned from the training data, data mining focuses on the discovery of (previously) unknown properties in the data (this is the analysis step of knowledge discovery in databases). The data is known as training data, and consists of a set of training examples.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "Df9rWjxf-wiW"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_similarity_matrix(sentences):\n",
        "    \"\"\"\n",
        "    Compute the similarity matrix for a list of sentences.\n",
        "\n",
        "    :param sentences: A list of sentences.\n",
        "    :return: A similarity matrix.\n",
        "    \"\"\"\n",
        "    # Transform sentences into a matrix of token counts\n",
        "    vectorizer = CountVectorizer().fit_transform(sentences)\n",
        "\n",
        "    # Compute cosine similarity between vectors\n",
        "    similarity_matrix = cosine_similarity(vectorizer)\n",
        "\n",
        "    return similarity_matrix\n"
      ],
      "metadata": {
        "id": "E-smbEIC-8Xs"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_text_cosine_similarity(text, target_length):\n",
        "    sentences = sent_tokenize(text)\n",
        "\n",
        "    if len(sentences) == 0:\n",
        "        return \"\"\n",
        "\n",
        "    # Compute cosine similarity matrix between sentences\n",
        "    similarity_matrix = compute_similarity_matrix(sentences)\n",
        "\n",
        "    length = 0\n",
        "    summary_sentences = []\n",
        "    selected_indices = set()\n",
        "\n",
        "    while length < target_length and len(selected_indices) < len(sentences):\n",
        "        if len(summary_sentences) == 0:\n",
        "            # If summary is empty, start with the first sentence\n",
        "            most_similar_index = 0\n",
        "        else:\n",
        "            # Find the sentence most similar to the existing summary\n",
        "            summary_indices = [i for i in range(len(sentences)) if i in selected_indices]\n",
        "            similarity_scores = similarity_matrix[summary_indices].sum(axis=0)\n",
        "            similarity_scores[list(selected_indices)] = 0  # Avoid selecting already chosen sentences\n",
        "            most_similar_index = np.argmax(similarity_scores)\n",
        "\n",
        "        if most_similar_index in selected_indices:\n",
        "            break\n",
        "\n",
        "        # Add the selected sentence to the summary\n",
        "        summary_sentences.append(sentences[most_similar_index])\n",
        "        selected_indices.add(most_similar_index)\n",
        "\n",
        "        # Update the length based on the added sentence\n",
        "        length += len(sentences[most_similar_index].split())\n",
        "\n",
        "    return ' '.join(summary_sentences).strip()"
      ],
      "metadata": {
        "id": "5hLx3X-n-8wT"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def perform_summarization(content, output_filename, context_window_limit):\n",
        "    length = len(content.split())\n",
        "    target_length = int(length * (context_window_limit / (context_window_limit + 4000)))\n",
        "\n",
        "    summary = summarize_text_cosine_similarity(content, target_length)\n",
        "\n",
        "    with open(output_filename, \"w\", encoding=\"utf-8\") as file:\n",
        "        file.write(summary)\n",
        "\n",
        "    return summary"
      ],
      "metadata": {
        "id": "QK3GacquAHRF"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Reading the content from input files\n",
        "    contents_1 = read_content_from_file(\"input_text1.txt\")\n",
        "    contents_2 = read_content_from_file(\"input_text2.txt\")\n",
        "\n",
        "    if contents_1 and contents_2:\n",
        "        # Perform summarization on both files\n",
        "        perform_summarization(contents_1, \"Second_summarized_text1.txt\", 128)\n",
        "        perform_summarization(contents_2, \"Second_summarized_text2.txt\", 128)\n",
        "\n",
        "        # Generate the query with the summarized content\n",
        "        query = f\"\\nDocument 1 summary: {read_content_from_file('Second_summarized_text1.txt')}\\n\" \\\n",
        "                f\"\\nDocument 2 summary: {read_content_from_file('Second_summarized_text2.txt')}\"\n",
        "\n",
        "        print(query)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYmrYWBtBXP9",
        "outputId": "77d51e93-ee7d-4525-93c8-9ba7a7646500"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Document 1 summary: Natural language processing (NLP) is an interdisciplinary subfield of computer science and information retrieval. Book generation\n",
            "Not an NLP task proper but an extension of natural language generation and other NLP tasks is the creation of full-fledged books. Common NLP tasks\n",
            "The following is a list of some of the most commonly researched tasks in natural language processing. As an example, George Lakoff offers a methodology to build natural language processing (NLP) algorithms through the perspective of cognitive science, along with the findings of cognitive linguistics, with two defining aspects:\n",
            "\n",
            "Apply the theory of conceptual metaphor, explained by Lakoff as \"the understanding of one idea, in terms of another\" which provides an idea of the intent of the author. The goal is a computer capable of \"understanding\" the contents of documents, including the contextual nuances of the language within them. The difficulty of this task depends greatly on the complexity of the morphology (i.e., the structure of words) of the language being considered.\n",
            "\n",
            "Document 2 summary: Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalize to unseen data, and thus perform tasks without explicit instructions. While responsible collection of data and documentation of algorithmic rules used by a system is considered a critical part of machine learning, some researchers blame lack of participation and representation of minority population in the field of AI for machine learning's vulnerability to biases. Reasons for this are numerous: lack of (suitable) data, lack of access to the data, data bias, privacy problems, badly chosen tasks and algorithms, wrong tools and people, lack of resources, and evaluation problems. Running machine learning model in embedded devices removes the need for transferring and storing data on cloud servers for further processing, henceforth, reducing data breaches and privacy leaks happening because of transferring data, and also minimizes theft of intellectual properties, personal data and business secrets. Instead of responding to feedback, unsupervised learning algorithms identify commonalities in the data and react based on the presence or absence of such commonalities in each new piece of data. Data compression\n",
            "Data mining\n",
            "Machine learning and data mining often employ the same methods and overlap significantly, but while machine learning focuses on prediction, based on known properties learned from the training data, data mining focuses on the discovery of (previously) unknown properties in the data (this is the analysis step of knowledge discovery in databases). The data is known as training data, and consists of a set of training examples.\n"
          ]
        }
      ]
    }
  ]
}