{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPsqGYe1HfGEfYtwKynSeZz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Amirhatamian/NLP/blob/main/LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install wikipedia-api\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_UwtIlViAmX",
        "outputId": "44de66ef-40bd-4ff1-d45d-84d551786831"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wikipedia-api\n",
            "  Downloading Wikipedia_API-0.6.0-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from wikipedia-api) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (2024.2.2)\n",
            "Installing collected packages: wikipedia-api\n",
            "Successfully installed wikipedia-api-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rHjMyCLhd7n0",
        "outputId": "41e7f0ef-9404-44ca-9ab0-88a40dad4922"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import wikipediaapi\n",
        "\n",
        "# Ensure required NLTK data is downloaded\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_wikipedia_content(subject):\n",
        "    # Initialize the Wikipedia API with the English language\n",
        "    wiki_wiki = wikipediaapi.Wikipedia('english')\n",
        "\n",
        "    # Fetch the Wikipedia page for the given subject\n",
        "    page_py = wiki_wiki.page(subject)\n",
        "\n",
        "    # Check if the page exists\n",
        "    if not page_py.exists():\n",
        "        print(f\"Wikipedia page for '{subject}' does not exist.\")\n",
        "        return None\n",
        "\n",
        "    # Return the text content of the page\n",
        "    return page_py.text\n",
        "\n",
        "# Example usage\n",
        "subject = \"Python (programming language)\"\n",
        "content = fetch_wikipedia_content(subject)\n",
        "if content:\n",
        "    print(content[:1000])  # Print the first 1000 characters of the content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_GyATZseWEV",
        "outputId": "84dab4b1-58aa-4fb4-925e-b09b398cd024"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python is a high-level, general-purpose programming language. Its design philosophy emphasizes code readability with the use of significant indentation.\n",
            "Python is dynamically typed and garbage-collected. It supports multiple programming paradigms, including structured (particularly procedural), object-oriented and functional programming. It is often described as a \"batteries included\" language due to its comprehensive standard library.\n",
            "Guido van Rossum began working on Python in the late 1980s as a successor to the ABC programming language and first released it in 1991 as Python 0.9.0. Python 2.0 was released in 2000. Python 3.0, released in 2008, was a major revision not completely backward-compatible with earlier versions. Python 2.7.18, released in 2020, was the last release of Python 2.\n",
            "Python consistently ranks as one of the most popular programming languages, and has gained widespread use in the machine learning community.\n",
            "\n",
            "History\n",
            "Python was invented in the late 1980s by Guido v\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def save_content_to_file(content, filename):\n",
        "    # Check if the content is not None\n",
        "    if content is None:\n",
        "        print(\"No content to save.\")\n",
        "        return\n",
        "\n",
        "    # Write the content to a file\n",
        "    with open(filename, 'w', encoding='utf-8') as file:\n",
        "        file.write(content)\n",
        "\n",
        "    print(f\"Content saved to {filename}\")\n",
        "\n",
        "# Example usage\n",
        "subject = \"Python (programming language)\"\n",
        "content = fetch_wikipedia_content(subject)\n",
        "if content:\n",
        "    save_content_to_file(content, \"python_programming_language.txt\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LSMofwwceWHF",
        "outputId": "908f2918-afeb-439a-bcec-a5ebd9d95875"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Content saved to python_programming_language.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def read_content_from_file(filename):\n",
        "    try:\n",
        "        # Open the file and read its content\n",
        "        with open(filename, 'r', encoding='utf-8') as file:\n",
        "            content = file.read()\n",
        "        return content\n",
        "    except FileNotFoundError:\n",
        "        print(f\"File '{filename}' not found.\")\n",
        "        return None\n",
        "\n",
        "# Example usage\n",
        "filename = \"python_programming_language.txt\"\n",
        "content = read_content_from_file(filename)\n",
        "if content:\n",
        "    print(content[:1000])  # Print the first 1000 characters of the content\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ygTyNnYDeWJy",
        "outputId": "2749c7d1-c516-4b6d-9c6f-3f1d4eac19ef"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python is a high-level, general-purpose programming language. Its design philosophy emphasizes code readability with the use of significant indentation.\n",
            "Python is dynamically typed and garbage-collected. It supports multiple programming paradigms, including structured (particularly procedural), object-oriented and functional programming. It is often described as a \"batteries included\" language due to its comprehensive standard library.\n",
            "Guido van Rossum began working on Python in the late 1980s as a successor to the ABC programming language and first released it in 1991 as Python 0.9.0. Python 2.0 was released in 2000. Python 3.0, released in 2008, was a major revision not completely backward-compatible with earlier versions. Python 2.7.18, released in 2020, was the last release of Python 2.\n",
            "Python consistently ranks as one of the most popular programming languages, and has gained widespread use in the machine learning community.\n",
            "\n",
            "History\n",
            "Python was invented in the late 1980s by Guido v\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('punkt')\n",
        "\n",
        "def summarize_text(text, max_sentences=5):\n",
        "    \"\"\"\n",
        "    Summarize the text to fit within the context window.\n",
        "\n",
        "    :param text: The text to be summarized.\n",
        "    :param max_sentences: The maximum number of sentences for the summary.\n",
        "    :return: The summarized text.\n",
        "    \"\"\"\n",
        "    # Tokenize the text into sentences\n",
        "    sentences = sent_tokenize(text)\n",
        "\n",
        "    # If the text is already short enough, return it as is\n",
        "    if len(sentences) <= max_sentences:\n",
        "        return text\n",
        "\n",
        "    # Simple summarization: take the first and last few sentences\n",
        "    summary = sentences[:max_sentences//2] + sentences[-max_sentences//2:]\n",
        "\n",
        "    return ' '.join(summary)\n",
        "\n",
        "# Example usage\n",
        "filename = \"python_programming_language.txt\"\n",
        "content = read_content_from_file(filename)\n",
        "if content:\n",
        "    summary = summarize_text(content, max_sentences=5)\n",
        "    print(summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VR_u22pxeWMU",
        "outputId": "2dec17b4-058c-48f7-9887-0bb729e1e3ed"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python is a high-level, general-purpose programming language. Its design philosophy emphasizes code readability with the use of significant indentation. O'Reilly Media. ISBN 978-1-4920-5632-4. External links\n",
            "\n",
            "Official website\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def perform_summarization(content, output_filename, context_window_limit):\n",
        "    \"\"\"\n",
        "    Summarize content to fit within the context window limit and save to file.\n",
        "\n",
        "    :param content: The text content to be summarized.\n",
        "    :param output_filename: The name of the file to save the summary.\n",
        "    :param context_window_limit: The maximum word count for the context window.\n",
        "    :return: The summarized text.\n",
        "    \"\"\"\n",
        "    length = len(content.split())\n",
        "    target_length = int(length * (context_window_limit / (context_window_limit + 4000)))\n",
        "\n",
        "    summary_list = []\n",
        "    start_index = 0\n",
        "    content_words = content.split()\n",
        "\n",
        "    while start_index < length:\n",
        "        end_index = start_index + target_length\n",
        "        slice_content = \" \".join(content_words[start_index:end_index])\n",
        "        summary_slice = summarize_text(slice_content, max_sentences=target_length//50)\n",
        "        summary_list.append(summary_slice)\n",
        "        start_index = end_index\n",
        "\n",
        "    collated_summary = \" \".join(summary_list)\n",
        "\n",
        "    while len(collated_summary.split()) > context_window_limit:\n",
        "        collated_summary = summarize_text(collated_summary, max_sentences=context_window_limit//50)\n",
        "\n",
        "    with open(output_filename, \"w\", encoding=\"utf-8\") as file:\n",
        "        file.write(collated_summary)\n",
        "\n",
        "    return collated_summary\n",
        "\n",
        "# Example usage\n",
        "filename = \"python_programming_language.txt\"\n",
        "content = read_content_from_file(filename)\n",
        "if content:\n",
        "    summarized_content = perform_summarization(content, \"summarized_python_programming_language.txt\", 200)\n",
        "    print(summarized_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H04YQA5PeWO2",
        "outputId": "bc17a431-5152-4f69-8947-6c7ff94a94ae"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python is a high-level, general-purpose programming language. Its design philosophy emphasizes code readability with the use of significant indentation. ISBN 978-1-4920-5632-4. External links Official website\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Main function\n",
        "def main():\n",
        "    # Take input from the user for the Wikipedia subjects\n",
        "    subject1 = input(\"Enter the first subject: \")\n",
        "    subject2 = input(\"Enter the second subject: \")\n",
        "\n",
        "    # Fetch content from Wikipedia\n",
        "    content1 = fetch_wikipedia_content(subject1)\n",
        "    content2 = fetch_wikipedia_content(subject2)\n",
        "\n",
        "    if content1 and content2:\n",
        "        # Save content to input files\n",
        "        save_content_to_file(content1, \"input_text1.txt\")\n",
        "        save_content_to_file(content2, \"input_text2.txt\")\n",
        "\n",
        "        # Summarize text files\n",
        "        summarized_content1 = perform_summarization(content1, \"summarized_text1.txt\", 128)\n",
        "        summarized_content2 = perform_summarization(content2, \"summarized_text2.txt\", 128)\n",
        "\n",
        "        # Generate the query\n",
        "        query = f\"\\nDocument 1 summary: {summarized_content1}\\n\\nDocument 2 summary: {summarized_content2}\"\n",
        "\n",
        "        print(query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        },
        "id": "T4_hcHAKeWRN",
        "outputId": "36066ff7-0824-413b-d746-c7e6277c1a11"
      },
      "execution_count": 13,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Content saved to input_text1.txt\n",
            "Content saved to input_text2.txt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-33acb0ecf82c>\u001b[0m in \u001b[0;36m<cell line: 26>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Run the main function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-13-33acb0ecf82c>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# Summarize text files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0msummarized_content1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperform_summarization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"summarized_text1.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0msummarized_content2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperform_summarization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"summarized_text2.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-739a20d69fa4>\u001b[0m in \u001b[0;36mperform_summarization\u001b[0;34m(content, output_filename, context_window_limit)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcollated_summary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mcontext_window_limit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mcollated_summary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msummarize_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcollated_summary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_sentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontext_window_limit\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-a35e34c21c9f>\u001b[0m in \u001b[0;36msummarize_text\u001b[0;34m(text, max_sentences)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \"\"\"\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# Tokenize the text into sentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# If the text is already short enough, return it as is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    105\u001b[0m     \"\"\"\n\u001b[1;32m    106\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"tokenizers/punkt/{language}.pickle\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1279\u001b[0m         \u001b[0mGiven\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m         \"\"\"\n\u001b[0;32m-> 1281\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1283\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdebug_decisions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mIterator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36msentences_from_text\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1339\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m         \"\"\"\n\u001b[0;32m-> 1341\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_last_whitespace_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the main function\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "Z844M-SO9SXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "Df9rWjxf-wiW"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_similarity_matrix(sentences):\n",
        "    \"\"\"\n",
        "    Compute the similarity matrix for a list of sentences.\n",
        "\n",
        "    :param sentences: A list of sentences.\n",
        "    :return: A similarity matrix.\n",
        "    \"\"\"\n",
        "    # Transform sentences into a matrix of token counts\n",
        "    vectorizer = CountVectorizer().fit_transform(sentences)\n",
        "\n",
        "    # Compute cosine similarity between vectors\n",
        "    similarity_matrix = cosine_similarity(vectorizer)\n",
        "\n",
        "    return similarity_matrix\n",
        "\n",
        "# Example usage\n",
        "sentences = [\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"A fast brown fox leaps over a lazy dog.\",\n",
        "    \"A quick dog jumps over a lazy fox.\",\n",
        "    \"The lazy dog lies in the sun.\"\n",
        "]\n",
        "\n",
        "similarity_matrix = compute_similarity_matrix(sentences)\n",
        "print(similarity_matrix)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-smbEIC-8Xs",
        "outputId": "5b2fd075-fa7e-4a5d-9f54-4e4540943328"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.         0.56980288 0.73854895 0.60302269]\n",
            " [0.56980288 1.         0.6172134  0.25197632]\n",
            " [0.73854895 0.6172134  1.         0.27216553]\n",
            " [0.60302269 0.25197632 0.27216553 1.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_text_cosine_similarity(text, target_length):\n",
        "    sentences = sent_tokenize(text)\n",
        "\n",
        "    if len(sentences) == 0:\n",
        "        return \"\"\n",
        "\n",
        "    # Compute cosine similarity matrix between sentences\n",
        "    similarity_matrix = compute_similarity_matrix(sentences)\n",
        "\n",
        "    length = 0\n",
        "    summary_sentences = []\n",
        "    selected_indices = set()\n",
        "\n",
        "    while length < target_length and len(selected_indices) < len(sentences):\n",
        "        if len(summary_sentences) == 0:\n",
        "            # If summary is empty, start with the first sentence\n",
        "            most_similar_index = 0\n",
        "        else:\n",
        "            # Find the sentence most similar to the existing summary\n",
        "            summary_indices = [i for i in range(len(sentences)) if i in selected_indices]\n",
        "            similarity_scores = similarity_matrix[summary_indices].sum(axis=0)\n",
        "            similarity_scores[list(selected_indices)] = 0  # Avoid selecting already chosen sentences\n",
        "            most_similar_index = np.argmax(similarity_scores)\n",
        "\n",
        "        if most_similar_index in selected_indices:\n",
        "            break\n",
        "\n",
        "        # Add the selected sentence to the summary\n",
        "        summary_sentences.append(sentences[most_similar_index])\n",
        "        selected_indices.add(most_similar_index)\n",
        "\n",
        "        # Update the length based on the added sentence\n",
        "        length += len(sentences[most_similar_index].split())\n",
        "\n",
        "    return ' '.join(summary_sentences).strip()\n",
        "\n",
        "# Example usage\n",
        "text = (\n",
        "    \"The quick brown fox jumps over the lazy dog. \"\n",
        "    \"A fast brown fox leaps over a lazy dog. \"\n",
        "    \"A quick dog jumps over a lazy fox. \"\n",
        "    \"The lazy dog lies in the sun. \"\n",
        "    \"The sun is shining brightly today. \"\n",
        "    \"Foxes are known for their quick movements.\"\n",
        ")\n",
        "summary = summarize_text_cosine_similarity(text, 50)\n",
        "print(summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5hLx3X-n-8wT",
        "outputId": "7ecad841-1413-4843-8c6a-a2277d9ed23c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The quick brown fox jumps over the lazy dog. A quick dog jumps over a lazy fox. A fast brown fox leaps over a lazy dog. The lazy dog lies in the sun. The sun is shining brightly today. Foxes are known for their quick movements.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def perform_summarization(content, output_filename, context_window_limit):\n",
        "    length = len(content.split())\n",
        "    target_length = int(length * (context_window_limit / (context_window_limit + 4000)))\n",
        "\n",
        "    summary = summarize_text_cosine_similarity(content, target_length)\n",
        "\n",
        "    with open(output_filename, \"w\", encoding=\"utf-8\") as file:\n",
        "        file.write(summary)\n",
        "\n",
        "    return summary\n",
        "\n",
        "# Example usage\n",
        "filename = \"python_programming_language.txt\"\n",
        "content = read_content_from_file(filename)\n",
        "if content:\n",
        "    summarized_content = perform_summarization(content, \"summarized_python_programming_language.txt\", 128)\n",
        "    print(summarized_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QK3GacquAHRF",
        "outputId": "7f9b8689-2db2-4fb1-d729-1fa52c81af4e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python is a high-level, general-purpose programming language. Design philosophy and features\n",
            "Python is a multi-paradigm programming language. Syntax and semantics\n",
            "Python is meant to be an easily readable language. It is also possible to cross-compile to other languages, but it either doesn't provide the full speed-up that might be expected, since Python is a very dynamic language, or a restricted subset of Python is compiled, and possibly semantics are slightly changed. Cross-compilers to other languages\n",
            "There are several compilers/transpilers to high-level object languages, with either unrestricted Python, a restricted subset of Python, or a language similar to Python as the source language:\n",
            "\n",
            "Brython, Transcrypt and Pyjs (latest release in 2012) compile Python to JavaScript. While Python 2.7 and older is officially unsupported, a different unofficial Python implementation, PyPy, continues to support Python 2, i.e. Examples of the use of this prefix in names of Python applications or libraries include Pygame, a binding of SDL to Python (commonly used to create games); PyQt and PyGTK, which bind Qt and GTK to Python respectively; and PyPy, a Python implementation originally written in Python. RPython can be compiled to C, and is used to build the PyPy interpreter of Python. Python 3.12 removed wstr meaning Python extensions need to be modified, and 3.10 added pattern matching to the language.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Reading the content from input files\n",
        "    contents_1 = read_content_from_file(\"input_text1.txt\")\n",
        "    contents_2 = read_content_from_file(\"input_text2.txt\")\n",
        "\n",
        "    if contents_1 and contents_2:\n",
        "        # Perform summarization on both files\n",
        "        perform_summarization(contents_1, \"Second_summarized_text1.txt\", 128)\n",
        "        perform_summarization(contents_2, \"Second_summarized_text2.txt\", 128)\n",
        "\n",
        "        # Generate the query with the summarized content\n",
        "        query = f\"\\nDocument 1 summary: {read_content_from_file('Second_summarized_text1.txt')}\\n\" \\\n",
        "                f\"\\nDocument 2 summary: {read_content_from_file('Second_summarized_text2.txt')}\"\n",
        "\n",
        "        print(query)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYmrYWBtBXP9",
        "outputId": "a91ed852-87e4-422f-cfc5-50e63368a1bd"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Document 1 summary: NLP commonly refers to:\n",
            "\n",
            "Natural language processing, a field of computer science and linguistics\n",
            "Neuro-linguistic programming, a pseudoscientific method aimed at modifying human behavior\n",
            "NLP or nLP may also refer to:\n",
            "\n",
            "Computer programming\n",
            "Natural-language programming, a programming paradigm\n",
            "\n",
            "Libraries\n",
            "National Library of Pakistan\n",
            "National Library of Poland\n",
            "National Library of the Philippines\n",
            "\n",
            "Mathematics\n",
            "Nonlinear programming, solving optimisation problems with nonlinear constraints\n",
            "\n",
            "Medicine\n",
            "No light perception, a diagnosis of severe blindness\n",
            "Nasopharyngeal Laryngoscope\n",
            "Lateral posterior nucleus of thalamus (nLP)\n",
            "\n",
            "Political Parties\n",
            "National Liberal Party (disambiguation)\n",
            "National Liberation Party (disambiguation)\n",
            "National Labour Party (disambiguation)\n",
            "Natural Law Party, a trans-national union of political parties, with national branches in over 80 countries\n",
            "Natural Law Party of Canada\n",
            "Natural Law Party (Ireland)\n",
            "Natural Law Party of Israel\n",
            "Natural Law Party of New Zealand\n",
            "Natural Law Party of Ontario\n",
            "Natural Law Party of Quebec\n",
            "Natural Law Party (Trinidad and Tobago)\n",
            "Natural Law Party (United States)\n",
            "\n",
            "Document 2 summary: Artificial intelligence (AI), in its broadest sense, is intelligence exhibited by machines, particularly computer systems. Artificial Intelligence. Artificial Intelligence (3rd ed.). \"Logic and Artificial Intelligence\". Artificial Intelligence: A Modern Approach (4th ed.). AI founder John McCarthy agreed, writing that \"Artificial intelligence is not, by definition, simulation of human intelligence\". General intelligence\n",
            "A machine with artificial general intelligence should be able to solve a wide variety of problems with breadth and versatility similar to human intelligence. Artificial intelligence laboratories were set up at a number of British and U.S. Regulation\n",
            "The regulation of artificial intelligence is the development of public sector policies and laws for promoting and regulating artificial intelligence (AI); it is therefore related to the broader regulation of algorithms. See also\n",
            "Artificial intelligence detection software – Software to detect AI-generated contentPages displaying short descriptions of redirect targets\n",
            "Behavior selection algorithm – Algorithm that selects actions for intelligent agents\n",
            "Business process automation – Technology-enabled automation of complex business processes\n",
            "Case-based reasoning – Process of solving new problems based on the solutions of similar past problems\n",
            "Computational intelligence – Ability of a computer to learn a specific task from data or experimental observation\n",
            "Digital immortality – Hypothetical concept of storing a personality in digital form\n",
            "Emergent algorithm – Algorithm exhibiting emergent behavior\n",
            "Female gendering of AI technologies – Gender biases in digital technologyPages displaying short descriptions of redirect targets\n",
            "Glossary of artificial intelligence – List of definitions of terms and concepts commonly used in the study of artificial intelligence\n",
            "Intelligence amplification – Use of information technology to augment human intelligence\n",
            "Mind uploading – Hypothetical process of digitally emulating a brain\n",
            "Robotic process automation – Form of business process automation technology\n",
            "Weak artificial intelligence – Form of artificial intelligence\n",
            "Wetware computer – Computer composed of organic material\n",
            "\n",
            "Explanatory notes\n",
            "References\n",
            "AI textbooks\n",
            "The two most widely used textbooks in 2023. The experimental sub-field of artificial general intelligence studies this area exclusively. The emerging field of neuro-symbolic artificial intelligence attempts to bridge the two approaches. This definition stipulates the ability of systems to synthesize information as the manifestation of intelligence, similar to the way it is defined in biological intelligence.\n"
          ]
        }
      ]
    }
  ]
}