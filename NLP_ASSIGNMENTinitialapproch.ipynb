{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPcwDqp3FIr/pwK0ZQsjnft",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Amirhatamian/NLP/blob/main/NLP_ASSIGNMENTinitialapproch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries\n",
        "!pip install wikipedia nltk scikit-learn PyPDF2 Wikipedia-API\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mt4cfLIQEryE",
        "outputId": "fb1a5d62-ebb2-4532-fb7c-3d2d9ec1a7ff"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wikipedia in /usr/local/lib/python3.10/dist-packages (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "Requirement already satisfied: Wikipedia-API in /usr/local/lib/python3.10/dist-packages (0.6.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (4.12.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (2.31.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2024.2.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->wikipedia) (2.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2dyZWL0mgqUT",
        "outputId": "df9ee869-1d9a-4191-86b5-3bed192de752"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "#Import Libraries and Download NLTK Data\n",
        "\n",
        "import re\n",
        "import string\n",
        "import numpy as np\n",
        "import wikipedia\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.probability import FreqDist\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import PyPDF2\n",
        "\n",
        "# Download NLTK data\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Wikipedia API for the English language\n",
        "wiki_wiki = wikipediaapi.Wikipedia('english')\n",
        "\n",
        "# Function to retrieve text from a Wikipedia page\n",
        "def get_wikipedia_text(page_title):\n",
        "    # Retrieve the page object for the given title\n",
        "    page = wiki_wiki.page(page_title)\n",
        "\n",
        "    # Check if the page exists\n",
        "    if not page.exists():\n",
        "        return None\n",
        "\n",
        "    # Return the text content of the page\n",
        "    return page.text\n"
      ],
      "metadata": {
        "id": "Uqlqr9pElj7D"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_keywords(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = word_tokenize(text)\n",
        "    filtered_words = [word.lower() for word in words if word.isalnum() and word.lower() not in stop_words]\n",
        "    return filtered_words\n"
      ],
      "metadata": {
        "id": "2Geephex6u6I"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_nouns(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = word_tokenize(text)\n",
        "    tagged_words = pos_tag(words)\n",
        "    nouns = [word.lower() for word, pos in tagged_words if pos.startswith('N') and word.lower() not in stop_words and word.isalnum()]\n",
        "    return nouns\n"
      ],
      "metadata": {
        "id": "qD5XVvi71uqY"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_wikipedia_text(topic):\n",
        "    try:\n",
        "        # Get the summary of the Wikipedia page for the topic\n",
        "        summary = wikipedia.summary(topic)\n",
        "        return summary\n",
        "    except wikipedia.exceptions.DisambiguationError as e:\n",
        "        # Handle disambiguation errors by choosing the first option\n",
        "        return wikipedia.summary(e.options[0])\n",
        "    except wikipedia.exceptions.PageError:\n",
        "        # Handle page errors (e.g., page not found)\n",
        "        return None\n",
        "\n",
        "def extract_nouns(text):\n",
        "    # Tokenize the text into words\n",
        "    words = word_tokenize(text)\n",
        "    # Tag words with part of speech\n",
        "    pos_tags = nltk.pos_tag(words)\n",
        "    # Extract nouns (NN, NNS, NNP, NNPS are the tags for different types of nouns)\n",
        "    nouns = [word for word, pos in pos_tags if pos in ['NN', 'NNS', 'NNP', 'NNPS']]\n",
        "    return nouns\n",
        "\n",
        "def extract_top_nouns(topics, num_top_nouns=10):\n",
        "    all_nouns = []\n",
        "\n",
        "    for topic in topics:\n",
        "        text = get_wikipedia_text(topic)\n",
        "        if text:\n",
        "            all_nouns.extend(extract_nouns(text))\n",
        "\n",
        "    # Remove common stopwords from the list of nouns\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_nouns = [noun for noun in all_nouns if noun.lower() not in stop_words]\n",
        "\n",
        "    # Create a frequency distribution of the nouns\n",
        "    nouns_freq_dist = FreqDist(filtered_nouns)\n",
        "    # Get the most common nouns\n",
        "    top_nouns = [word for word, _ in nouns_freq_dist.most_common(num_top_nouns)]\n",
        "\n",
        "    return top_nouns"
      ],
      "metadata": {
        "id": "8cePXfTl1uz6"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "geographic_topics = ['New York', 'Mount Everest', 'Sahara Desert', 'Amazon River', 'Paris']\n",
        "non_geographic_topics = ['Quantum Mechanics', 'Artificial Intelligence', 'Shakespeare', 'Modern Art', 'Jazz Music']\n"
      ],
      "metadata": {
        "id": "m4xdgP8Q1u2f"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_geographic_keywords = extract_top_nouns(geographic_topics, num_top_nouns=10)\n",
        "top_non_geographic_keywords = extract_top_nouns(non_geographic_topics, num_top_nouns=10)\n",
        "\n",
        "print(\"Top geographic keywords:\", top_geographic_keywords)\n",
        "print(\"Top non-geographic keywords:\", top_non_geographic_keywords)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gYwWnvnk1u5M",
        "outputId": "84cea975-469a-4e91-9baf-7494d8f87149"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
            "\n",
            "The code that caused this warning is on line 389 of the file /usr/local/lib/python3.10/dist-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
            "\n",
            "  lis = BeautifulSoup(html).find_all('li')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top geographic keywords: ['New', 'York', 'Paris', 'city', 'world', 'state', 'River', 'Amazon', 'United', 'area']\n",
            "Top non-geographic keywords: ['AI', 'jazz', 'art', 'Shakespeare', 'systems', 'intelligence', 'works', 'physics', 'quantum', 'mechanics']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine all topics\n",
        "all_topics = geographic_topics + non_geographic_topics\n",
        "all_docs = []\n",
        "all_labels = []\n",
        "\n",
        "# Fetch text and extract nouns for each topic\n",
        "for topic in all_topics:\n",
        "    text = get_wikipedia_text(topic)\n",
        "    if text:\n",
        "        nouns = extract_nouns(text)\n",
        "        all_docs.append(\" \".join(nouns))\n",
        "        # Label as 1 for geographic and 0 for non-geographic\n",
        "        all_labels.append(1 if topic in geographic_topics else 0)\n",
        "\n",
        "# Combine top keywords from both categories\n",
        "all_top_keywords = top_geographic_keywords + top_non_geographic_keywords\n",
        "\n",
        "# Vectorize the documents using the top keywords\n",
        "vectorizer = CountVectorizer(vocabulary=all_top_keywords)\n",
        "X = vectorizer.transform(all_docs)\n",
        "y = all_labels\n",
        "\n",
        "print(\"Feature matrix shape:\", X.shape)\n",
        "print(\"Labels:\", y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FqlMtd1Z1vD8",
        "outputId": "0206c001-c13e-49c0-863e-91549c3ddb42"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature matrix shape: (9, 20)\n",
            "Labels: [1, 1, 1, 1, 0, 0, 0, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "7Nw4fejY1vGe"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Majority class naive classifier\n",
        "class NaiveClassifier:\n",
        "    def __init__(self):\n",
        "        self.majority_class = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        unique_classes, counts = np.unique(y, return_counts=True)\n",
        "        self.majority_class = unique_classes[np.argmax(counts)]\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.full(X.shape[0], self.majority_class)\n",
        "\n",
        "# Instantiate and train the naive classifier\n",
        "naive_classifier = NaiveClassifier()\n",
        "naive_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "naive_predictions = naive_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, naive_predictions)\n",
        "report = classification_report(y_test, naive_predictions, target_names=['non-geographic', 'geographic'])\n",
        "\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "print('Classification Report:')\n",
        "print(report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wd2mWTYr1vJQ",
        "outputId": "a13a589b-bc2a-4b7a-9e29-e1f87442bda4"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 50.00%\n",
            "Classification Report:\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "non-geographic       0.50      1.00      0.67         1\n",
            "    geographic       0.00      0.00      0.00         1\n",
            "\n",
            "      accuracy                           0.50         2\n",
            "     macro avg       0.25      0.50      0.33         2\n",
            "  weighted avg       0.25      0.50      0.33         2\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the path to the PDF file on your desktop\n",
        "pdf_file_path = \"path_to_your_pdf_file.pdf\"\n",
        "\n",
        "# Read the content of the PDF file\n",
        "with open(pdf_file_path, 'rb') as file:\n",
        "    pdf_reader = PyPDF2.PdfReader(file)\n",
        "    pdf_text = ''\n",
        "    for page_num in range(len(pdf_reader.pages)):\n",
        "        page = pdf_reader.pages[page_num]\n",
        "        pdf_text += page.extract_text()\n",
        "\n",
        "# Vectorize the PDF document\n",
        "document_vectorized = vectorize_document(pdf_text, vectorizer)\n",
        "\n",
        "# Predict the class of the PDF document\n",
        "naive_prediction = naive_classifier.predict(document_vectorized)\n",
        "\n",
        "if naive_prediction[0] == 1:\n",
        "    print('The document is classified as geographic.')\n",
        "else:\n",
        "    print('The document is classified as non-geographic.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "0SVdZr1Z_f12",
        "outputId": "4aba8ac2-2d38-4684-cb15-23c7089f2a2f"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'path_to_your_pdf_file.pdf'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-004a4c8c9dc2>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Read the content of the PDF file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdf_file_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mpdf_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPyPDF2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPdfReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mpdf_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'path_to_your_pdf_file.pdf'"
          ]
        }
      ]
    }
  ]
}